<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Compare Analysis Runs</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap-icons@1.11.3/font/bootstrap-icons.min.css">
    <style>
        body {
            font-family: system-ui, -apple-system, "Segoe UI", Roboto, "Helvetica Neue", "Noto Sans", "Liberation Sans", Arial, sans-serif;
            background-color: #f8f9fa;
        }
        .card {
            border: 1px solid #dee2e6;
            box-shadow: 0 0.125rem 0.25rem rgba(0, 0, 0, 0.075);
        }
        .llm-analysis-content {
            font-size: 0.9em;
        }
        #llmComparisonOutput pre {
            background-color: #212529;
            color: #f8f9fa;
            padding: 1rem;
            border-radius: 0.375rem;
        }
    </style>
</head>
<body>
    <div class="container my-4">
        <h1 class="mb-4 pb-3 border-bottom"><a href="/" class="text-decoration-none text-dark"><i class="bi bi-arrow-left-circle"></i></a> Comparison of Runs</h1>

        <div class="card mb-4">
            <div class="card-header fs-5">
                <i class="bi bi-robot"></i> AI-Powered Comparison Summary
            </div>
            <div class="card-body">
                <div class="mb-3">
                    <label for="customQuestion" class="form-label">Ask a specific question about these runs (optional):</label>
                    <textarea class="form-control" id="customQuestion" rows="2" placeholder="e.g., Which run shows a more severe memory leak?"></textarea>
                </div>
                <button id="getLlmComparisonBtn" class="btn btn-primary">
                    <i class="bi bi-lightbulb"></i> Analyze Differences with LLM
                </button>
                <div id="llmComparisonOutput" class="mt-3 p-3 border rounded bg-white" style="min-height: 100px; white-space: pre-wrap; word-wrap: break-word;">
                    <p class="text-muted"><em>Click the button above to get an LLM-generated comparison of the selected runs.</em></p>
                </div>
            </div>
        </div>

        {% if runs_to_compare %}
            <div class="row">
                {% for run_data in runs_to_compare %}
                    <div class="col-lg-6 mb-4">
                        <div class="card h-100">
                            <div class="card-header">
                                <h5><a href="{{ url_for('view_run', run=run_data.name) }}">{{ run_data.name }}</a></h5>
                            </div>
                            <div class="card-body">
                                {% if run_data.error %}
                                    <p class="text-danger">Error loading data for this run: {{ run_data.error }}</p>
                                {% else %}
                                    <p class="mb-1"><strong>HPROF:</strong> <small class="text-muted">{{ run_data.hprof_source | default('N/A') }}</small></p>
                                    <p class="mb-1"><strong>Timestamp:</strong> {{ run_data.timestamp | default('N/A') }}</p>
                                    <p class="mb-1"><strong>Model Used:</strong> {{ run_data.model_used | default('N/A') }}</p>
                                    <p class="mb-1"><strong>MAT Report:</strong> {{ run_data.mat_report_type | default('N/A') }}</p>
                                    
                                    <h6 class="mt-3">LLM Analysis:</h6>
                                    <div class="llm-analysis-content border rounded p-2" style="max-height: 300px; overflow-y: auto;">
                                        {{ run_data.llm_analysis_html | default('<p><em>LLM analysis not available.</em></p>') | safe }}
                                    </div>
                                {% endif %}
                            </div>
                        </div>
                    </div>
                {% endfor %}
            </div>
        {% else %}
            <div class="alert alert-warning">No data available for comparison, or not enough runs selected/found.</div>
        {% endif %}
    </div>

    <script>
        // --- This entire JavaScript section is unchanged from the previous version ---
        // It handles the LLM comparison logic.
        // No modifications are needed here for the visual redesign.
        document.addEventListener('DOMContentLoaded', function() {
            const getLlmComparisonBtn = document.getElementById('getLlmComparisonBtn');
            const llmComparisonOutputDiv = document.getElementById('llmComparisonOutput');
            const customQuestionInput = document.getElementById('customQuestion');
            
            const runsDataForComparison = {{ runs_to_compare | tojson | safe }};
            const validRuns = runsDataForComparison.filter(r => !r.error);

            if (getLlmComparisonBtn) {
                if (validRuns.length < 1) {
                    getLlmComparisonBtn.disabled = true;
                    getLlmComparisonBtn.title = "Need at least one valid run to analyze.";
                }

                getLlmComparisonBtn.addEventListener('click', function() {
                    if (validRuns.length < 1) {
                        llmComparisonOutputDiv.innerHTML = '<p class="text-danger">Not enough valid run data available for LLM comparison.</p>';
                        return;
                    }

                    llmComparisonOutputDiv.innerHTML = '<p class="text-muted"><em><i class="bi bi-hourglass-split"></i> Contacting LLM for comparison... Please wait.</em></p>';
                    getLlmComparisonBtn.disabled = true;
                    getLlmComparisonBtn.innerHTML = '<i class="bi bi-hourglass-split"></i> Analyzing...';
                    const customQuestion = customQuestionInput.value.trim();

                    const comparisonPayload = {
                        runs: validRuns.map(run => ({
                            name: run.name,
                            hprof_source: run.hprof_source,
                            model_used: run.model_used,
                            mat_report_type: run.mat_report_type,
                            raw_llm_analysis_text: run.raw_llm_analysis_text,
                            raw_thread_dump_text: run.raw_thread_dump_text
                        })),
                        custom_question: customQuestion || null
                    };
                    
                    fetch('/api/llm_compare_runs', {
                        method: 'POST',
                        headers: {'Content-Type': 'application/json'},
                        body: JSON.stringify(comparisonPayload)
                    })
                    .then(response => {
                        if (!response.ok) {
                            return response.json().then(errData => { throw new Error(errData.error || `HTTP error! Status: ${response.status}`) });
                        }
                        return response.json();
                    })
                    .then(data => {
                        if (data.success && data.comparison_analysis) {
                            let htmlContent = data.comparison_analysis.replace(/&/g, "&").replace(/</g, "<").replace(/>/g, ">");
                            htmlContent = htmlContent.replace(/\*\*(.*?)\*\*/g, '<strong>$1</strong>');
                            htmlContent = htmlContent.replace(/```([\s\S]*?)```/g, (match, p1) => `<pre class="bg-dark text-light p-2 rounded"><code>${p1.trim()}</code></pre>`);
                            htmlContent = htmlContent.replace(/`([^`]+)`/g, '<code>$1</code>');
                            llmComparisonOutputDiv.innerHTML = htmlContent.replace(/\n/g, '<br>');
                        } else {
                            llmComparisonOutputDiv.innerHTML = `<p class="text-danger">Error from LLM comparison: ${data.error || 'Unknown error.'}</p>`;
                        }
                    })
                    .catch(error => {
                        console.error('Error fetching LLM comparison:', error);
                        llmComparisonOutputDiv.innerHTML = `<p class="text-danger">Client-side error fetching LLM comparison: ${error.message}</p>`;
                    })
                    .finally(() => {
                        getLlmComparisonBtn.disabled = false;
                        getLlmComparisonBtn.innerHTML = '<i class="bi bi-lightbulb"></i> Analyze Differences with LLM';
                    });
                });
            }
        });
    </script>
</body>
</html>